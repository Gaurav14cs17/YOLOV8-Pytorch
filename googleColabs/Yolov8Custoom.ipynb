{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lvh2dj6qjsbP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "from torch.nn.functional import cross_entropy, one_hot\n",
        "\n",
        "# model.py - Fix the RepViTBlock for mixed precision\n",
        "\n",
        "class RepViTBlock(nn.Module):\n",
        "    def __init__(self, in_channels, se_ratio=0.25, stride=1):\n",
        "        super().__init__()\n",
        "        self.stride = stride\n",
        "        self.in_channels = in_channels\n",
        "        self.fused = False\n",
        "\n",
        "        # depthwise 3x3\n",
        "        self.rbr_dense = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, 3, stride, 1, groups=in_channels, bias=False),\n",
        "            nn.BatchNorm2d(in_channels)\n",
        "        )\n",
        "\n",
        "        # depthwise 1x1\n",
        "        self.rbr_1x1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, 1, stride, 0, groups=in_channels, bias=False),\n",
        "            nn.BatchNorm2d(in_channels)\n",
        "        )\n",
        "\n",
        "        # pointwise conv\n",
        "        self.pwconv = nn.Conv2d(in_channels, in_channels, 1, bias=False)\n",
        "        self.bn_pw = nn.BatchNorm2d(in_channels)\n",
        "\n",
        "        # SE block\n",
        "        self.se = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_channels, max(1, int(in_channels * se_ratio)), 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(max(1, int(in_channels * se_ratio)), in_channels, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.shortcut = (stride == 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Check if we're using reparameterized version\n",
        "        if hasattr(self, 'rbr_reparam'):\n",
        "            # Using reparameterized convolution\n",
        "            if self.rbr_reparam.weight.device != x.device:\n",
        "                self.rbr_reparam = self.rbr_reparam.to(x.device)\n",
        "            if self.rbr_reparam.weight.dtype != x.dtype:\n",
        "                self.rbr_reparam = self.rbr_reparam.to(x.dtype)\n",
        "            y = self.rbr_reparam(x)\n",
        "        else:\n",
        "            # Using original branches - ensure they're on correct device\n",
        "            if not self.fused:\n",
        "                if self.rbr_dense[0].weight.device != x.device:\n",
        "                    self.rbr_dense = self.rbr_dense.to(x.device)\n",
        "                if self.rbr_1x1[0].weight.device != x.device:\n",
        "                    self.rbr_1x1 = self.rbr_1x1.to(x.device)\n",
        "\n",
        "            y = self.rbr_dense(x) + self.rbr_1x1(x)\n",
        "\n",
        "        # Ensure other components are on correct device\n",
        "        if self.pwconv.weight.device != x.device:\n",
        "            self.pwconv = self.pwconv.to(x.device)\n",
        "            self.bn_pw = self.bn_pw.to(x.device)\n",
        "        if self.se[1].weight.device != x.device:\n",
        "            self.se = self.se.to(x.device)\n",
        "\n",
        "        y = self.pwconv(y)\n",
        "        y = self.bn_pw(y)\n",
        "        y = y * self.se(y)\n",
        "        y = F.relu(y)\n",
        "        return x + y if self.shortcut else y\n",
        "\n",
        "    def _fuse_reparam(self):\n",
        "        # Ensure fusion happens on the correct device\n",
        "        device = self.rbr_dense[0].weight.device\n",
        "        k3, b3 = self._fuse_conv_bn(self.rbr_dense)\n",
        "        k1, b1 = self._fuse_conv_bn(self.rbr_1x1)\n",
        "        k1_pad = F.pad(k1, [1, 1, 1, 1])\n",
        "        fused_k = k3 + k1_pad\n",
        "        fused_b = b3 + b1\n",
        "\n",
        "        self.rbr_reparam = nn.Conv2d(\n",
        "            self.in_channels, self.in_channels, kernel_size=3,\n",
        "            stride=self.stride, padding=1, groups=self.in_channels, bias=True\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.rbr_reparam.weight.copy_(fused_k)\n",
        "            self.rbr_reparam.bias.copy_(fused_b)\n",
        "\n",
        "        # Only delete if they exist (for safety)\n",
        "        if hasattr(self, 'rbr_dense'):\n",
        "            del self.rbr_dense\n",
        "        if hasattr(self, 'rbr_1x1'):\n",
        "            del self.rbr_1x1\n",
        "        self.fused = True\n",
        "\n",
        "    @staticmethod\n",
        "    def _fuse_conv_bn(branch):\n",
        "        conv = branch[0]\n",
        "        bn = branch[1]\n",
        "        w = conv.weight\n",
        "        if conv.bias is None:\n",
        "            bias = torch.zeros(w.size(0), device=w.device)\n",
        "        else:\n",
        "            bias = conv.bias\n",
        "\n",
        "        bn_var_rsqrt = 1.0 / torch.sqrt(bn.running_var + bn.eps)\n",
        "        w_fused = w * (bn.weight * bn_var_rsqrt).reshape(-1, 1, 1, 1)\n",
        "        b_fused = bn.bias + (bias - bn.running_mean) * bn_var_rsqrt * bn.weight\n",
        "        return w_fused, b_fused\n",
        "\n",
        "    def fuse(self):\n",
        "        \"\"\"Fuse the block for inference\"\"\"\n",
        "        if not self.fused:\n",
        "            self._fuse_reparam()\n",
        "        return self\n",
        "\n",
        "# =========================================================\n",
        "# RepViT Backbone\n",
        "# =========================================================\n",
        "class RepViTBackbone(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.stage1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            RepViTBlock(32)\n",
        "        )\n",
        "        self.stage2 = nn.Sequential(\n",
        "            RepViTBlock(32, stride=2),\n",
        "            RepViTBlock(32)\n",
        "        )\n",
        "        self.stage3 = nn.Sequential(\n",
        "            RepViTBlock(32, stride=2),\n",
        "            RepViTBlock(32),\n",
        "            RepViTBlock(32)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        c2 = self.stage1(x)\n",
        "        c3 = self.stage2(c2)\n",
        "        c4 = self.stage3(c3)\n",
        "        return [c2, c3, c4]\n",
        "\n",
        "# =========================================================\n",
        "# Ghost Module\n",
        "# =========================================================\n",
        "class GhostModule(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, ratio=2, kernel_size=1, dw_size=3, stride=1, relu=True):\n",
        "        super().__init__()\n",
        "        init_ch = out_ch // ratio\n",
        "        new_ch = out_ch - init_ch\n",
        "        self.primary = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, init_ch, kernel_size, stride, kernel_size // 2, bias=False),\n",
        "            nn.BatchNorm2d(init_ch),\n",
        "            nn.ReLU(inplace=True) if relu else nn.Identity()\n",
        "        )\n",
        "        self.cheap = nn.Sequential(\n",
        "            nn.Conv2d(init_ch, new_ch, dw_size, 1, dw_size // 2, groups=init_ch, bias=False),\n",
        "            nn.BatchNorm2d(new_ch),\n",
        "            nn.ReLU(inplace=True) if relu else nn.Identity()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.primary(x)\n",
        "        z = self.cheap(y)\n",
        "        return torch.cat([y, z], dim=1)\n",
        "\n",
        "# =========================================================\n",
        "# GhostNeck (multi-scale features)\n",
        "# =========================================================\n",
        "class GhostNeck(nn.Module):\n",
        "    def __init__(self, chs=[32, 32, 32]):\n",
        "        super().__init__()\n",
        "        c2, c3, c4 = chs\n",
        "        self.reduce_c4 = GhostModule(c4, 64)            # p5 channels = 64\n",
        "        self.reduce_c3 = GhostModule(c3 + 64, 48)       # p4 channels = 48\n",
        "        self.reduce_c2 = GhostModule(c2 + 48, 32)       # p3 channels = 32\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "\n",
        "    def forward(self, feats):\n",
        "        c2, c3, c4 = feats\n",
        "        p5 = self.reduce_c4(c4)\n",
        "        p4 = self.reduce_c3(torch.cat([self.up(p5), c3], dim=1))\n",
        "        p3 = self.reduce_c2(torch.cat([self.up(p4), c2], dim=1))\n",
        "        return p3, p4, p5\n",
        "\n",
        "# =========================================================\n",
        "# GhostHead (detection head)\n",
        "# =========================================================\n",
        "class GhostHead(nn.Module):\n",
        "    def __init__(self, in_ch, num_classes):\n",
        "        super().__init__()\n",
        "        self.conv = GhostModule(in_ch, 32)\n",
        "        # Output: 4 box coordinates + num_classes\n",
        "        self.pred = nn.Conv2d(32, 4 + num_classes, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.pred(x)\n",
        "        return x\n",
        "\n",
        "# =========================================================\n",
        "# YOLOv8Hybrid (single or multi-head)\n",
        "# =========================================================\n",
        "class YOLOv8Hybrid(nn.Module):\n",
        "    def __init__(self, num_classes=80, multi_head=False):\n",
        "        super().__init__()\n",
        "        self.multi_head = multi_head\n",
        "        self.num_classes = num_classes\n",
        "        self.backbone = RepViTBackbone()\n",
        "        self.neck = GhostNeck([32, 32, 32])\n",
        "\n",
        "        if multi_head:\n",
        "            self.head_p3 = GhostHead(32, num_classes)\n",
        "            self.head_p4 = GhostHead(48, num_classes)\n",
        "            self.head_p5 = GhostHead(64, num_classes)\n",
        "        else:\n",
        "            # project p4/p5 -> p3 channels before summation\n",
        "            self.proj_p4 = nn.Conv2d(48, 32, 1, bias=False)\n",
        "            self.bn_p4 = nn.BatchNorm2d(32)\n",
        "            self.proj_p5 = nn.Conv2d(64, 32, 1, bias=False)\n",
        "            self.bn_p5 = nn.BatchNorm2d(32)\n",
        "            self.head = GhostHead(32, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        p3, p4, p5 = self.neck(self.backbone(x))\n",
        "\n",
        "        if self.multi_head:\n",
        "            out_p3 = self.head_p3(p3)\n",
        "            out_p4 = self.head_p4(p4)\n",
        "            out_p5 = self.head_p5(p5)\n",
        "            return [out_p3, out_p4, out_p5]\n",
        "        else:\n",
        "            p4_up = F.interpolate(p4, size=p3.shape[2:], mode='nearest')\n",
        "            p5_up = F.interpolate(p5, size=p3.shape[2:], mode='nearest')\n",
        "\n",
        "            p4_proj = self.bn_p4(self.proj_p4(p4_up))\n",
        "            p5_proj = self.bn_p5(self.proj_p5(p5_up))\n",
        "\n",
        "            fused = p3 + p4_proj + p5_proj\n",
        "            return self.head(fused)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.nn.functional import cross_entropy, one_hot\n",
        "\n",
        "\n",
        "def setup_seed():\n",
        "    \"\"\"\n",
        "    Setup random seed.\n",
        "    \"\"\"\n",
        "    random.seed(0)\n",
        "    numpy.random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def setup_multi_processes():\n",
        "    \"\"\"\n",
        "    Setup multi-processing environment variables.\n",
        "    \"\"\"\n",
        "    import cv2\n",
        "    from os import environ\n",
        "    from platform import system\n",
        "\n",
        "    # set multiprocess start method as `fork` to speed up the training\n",
        "    if system() != 'Windows':\n",
        "        torch.multiprocessing.set_start_method('fork', force=True)\n",
        "\n",
        "    # disable opencv multithreading to avoid system being overloaded\n",
        "    cv2.setNumThreads(0)\n",
        "\n",
        "    # setup OMP threads\n",
        "    if 'OMP_NUM_THREADS' not in environ:\n",
        "        environ['OMP_NUM_THREADS'] = '1'\n",
        "\n",
        "    # setup MKL threads\n",
        "    if 'MKL_NUM_THREADS' not in environ:\n",
        "        environ['MKL_NUM_THREADS'] = '1'\n",
        "\n",
        "\n",
        "def scale(coords, shape1, gain, pad):\n",
        "    coords[:, [0, 2]] -= pad[0]  # x padding\n",
        "    coords[:, [1, 3]] -= pad[1]  # y padding\n",
        "    coords[:, :4] /= gain[0]  # gain_x == gain_y for letterbox\n",
        "    coords[:, 0].clamp_(0, shape1[1])  # x1\n",
        "    coords[:, 1].clamp_(0, shape1[0])  # y1\n",
        "    coords[:, 2].clamp_(0, shape1[1])  # x2\n",
        "    coords[:, 3].clamp_(0, shape1[0])  # y2\n",
        "    return coords\n",
        "\n",
        "\n",
        "def make_anchors(x, strides, offset=0.5):\n",
        "    \"\"\"\n",
        "    Generate anchors from features\n",
        "    \"\"\"\n",
        "    assert x is not None\n",
        "    anchor_points, stride_tensor = [], []\n",
        "    for i, stride in enumerate(strides):\n",
        "        _, _, h, w = x[i].shape\n",
        "        sx = torch.arange(end=w, dtype=x[i].dtype, device=x[i].device) + offset  # shift x\n",
        "        sy = torch.arange(end=h, dtype=x[i].dtype, device=x[i].device) + offset  # shift y\n",
        "        sy, sx = torch.meshgrid(sy, sx)\n",
        "        anchor_points.append(torch.stack((sx, sy), -1).view(-1, 2))\n",
        "        stride_tensor.append(torch.full((h * w, 1), stride, dtype=x[i].dtype, device=x[i].device))\n",
        "    return torch.cat(anchor_points), torch.cat(stride_tensor)\n",
        "\n",
        "\n",
        "def box_iou(box1, box2):\n",
        "    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n",
        "    \"\"\"\n",
        "    Return intersection-over-union (Jaccard index) of boxes.\n",
        "    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
        "    Arguments:\n",
        "        box1 (Tensor[N, 4])\n",
        "        box2 (Tensor[M, 4])\n",
        "    Returns:\n",
        "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
        "            IoU values for every element in boxes1 and boxes2\n",
        "    \"\"\"\n",
        "\n",
        "    # intersection(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
        "    (a1, a2), (b1, b2) = box1[:, None].chunk(2, 2), box2.chunk(2, 1)\n",
        "    intersection = (torch.min(a2, b2) - torch.max(a1, b1)).clamp(0).prod(2)\n",
        "\n",
        "    # IoU = intersection / (area1 + area2 - intersection)\n",
        "    box1 = box1.T\n",
        "    box2 = box2.T\n",
        "\n",
        "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "\n",
        "    return intersection / (area1[:, None] + area2 - intersection)\n",
        "\n",
        "\n",
        "def wh2xy(x):\n",
        "    y = x.clone()\n",
        "    y[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\n",
        "    y[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\n",
        "    y[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\n",
        "    y[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\n",
        "    return y\n",
        "\n",
        "\n",
        "def non_max_suppression(prediction, conf_threshold=0.25, iou_threshold=0.45):\n",
        "    nc = prediction.shape[1] - 4  # number of classes\n",
        "    xc = prediction[:, 4:4 + nc].amax(1) > conf_threshold  # candidates\n",
        "\n",
        "    # Settings\n",
        "    max_wh = 7680  # (pixels) maximum box width and height\n",
        "    max_det = 300  # the maximum number of boxes to keep after NMS\n",
        "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "\n",
        "    start = time.time()\n",
        "    outputs = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "    for index, x in enumerate(prediction):  # image index, image inference\n",
        "        # Apply constraints\n",
        "        x = x.transpose(0, -1)[xc[index]]  # confidence\n",
        "\n",
        "        # If none remain process next image\n",
        "        if not x.shape[0]:\n",
        "            continue\n",
        "\n",
        "        # Detections matrix nx6 (box, conf, cls)\n",
        "        box, cls = x.split((4, nc), 1)\n",
        "        # center_x, center_y, width, height) to (x1, y1, x2, y2)\n",
        "        box = wh2xy(box)\n",
        "        if nc > 1:\n",
        "            i, j = (cls > conf_threshold).nonzero(as_tuple=False).T\n",
        "            x = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float()), 1)\n",
        "        else:  # best class only\n",
        "            conf, j = cls.max(1, keepdim=True)\n",
        "            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_threshold]\n",
        "        # Check shape\n",
        "        if not x.shape[0]:  # no boxes\n",
        "            continue\n",
        "        # sort by confidence and remove excess boxes\n",
        "        x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
        "\n",
        "        # Batched NMS\n",
        "        c = x[:, 5:6] * max_wh  # classes\n",
        "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "        i = torchvision.ops.nms(boxes, scores, iou_threshold)  # NMS\n",
        "        i = i[:max_det]  # limit detections\n",
        "        outputs[index] = x[i]\n",
        "        if (time.time() - start) > 0.5 + 0.05 * prediction.shape[0]:\n",
        "            print(f'WARNING ⚠️ NMS time limit {0.5 + 0.05 * prediction.shape[0]:.3f}s exceeded')\n",
        "            break  # time limit exceeded\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def smooth(y, f=0.05):\n",
        "    # Box filter of fraction f\n",
        "    nf = round(len(y) * f * 2) // 2 + 1  # number of filter elements (must be odd)\n",
        "    p = numpy.ones(nf // 2)  # ones padding\n",
        "    yp = numpy.concatenate((p * y[0], y, p * y[-1]), 0)  # y padded\n",
        "    return numpy.convolve(yp, numpy.ones(nf) / nf, mode='valid')  # y-smoothed\n",
        "\n",
        "\n",
        "def compute_ap(tp, conf, pred_cls, target_cls, eps=1e-16):\n",
        "    \"\"\"\n",
        "    Compute the average precision, given the recall and precision curves.\n",
        "    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
        "    # Arguments\n",
        "        tp:  True positives (nparray, nx1 or nx10).\n",
        "        conf:  Object-ness value from 0-1 (nparray).\n",
        "        pred_cls:  Predicted object classes (nparray).\n",
        "        target_cls:  True object classes (nparray).\n",
        "    # Returns\n",
        "        The average precision\n",
        "    \"\"\"\n",
        "    # Sort by object-ness\n",
        "    i = numpy.argsort(-conf)\n",
        "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
        "\n",
        "    # Find unique classes\n",
        "    unique_classes, nt = numpy.unique(target_cls, return_counts=True)\n",
        "    nc = unique_classes.shape[0]  # number of classes, number of detections\n",
        "\n",
        "    # Create Precision-Recall curve and compute AP for each class\n",
        "    p = numpy.zeros((nc, 1000))\n",
        "    r = numpy.zeros((nc, 1000))\n",
        "    ap = numpy.zeros((nc, tp.shape[1]))\n",
        "    px, py = numpy.linspace(0, 1, 1000), []  # for plotting\n",
        "    for ci, c in enumerate(unique_classes):\n",
        "        i = pred_cls == c\n",
        "        nl = nt[ci]  # number of labels\n",
        "        no = i.sum()  # number of outputs\n",
        "        if no == 0 or nl == 0:\n",
        "            continue\n",
        "\n",
        "        # Accumulate FPs and TPs\n",
        "        fpc = (1 - tp[i]).cumsum(0)\n",
        "        tpc = tp[i].cumsum(0)\n",
        "\n",
        "        # Recall\n",
        "        recall = tpc / (nl + eps)  # recall curve\n",
        "        # negative x, xp because xp decreases\n",
        "        r[ci] = numpy.interp(-px, -conf[i], recall[:, 0], left=0)\n",
        "\n",
        "        # Precision\n",
        "        precision = tpc / (tpc + fpc)  # precision curve\n",
        "        p[ci] = numpy.interp(-px, -conf[i], precision[:, 0], left=1)  # p at pr_score\n",
        "\n",
        "        # AP from recall-precision curve\n",
        "        for j in range(tp.shape[1]):\n",
        "            m_rec = numpy.concatenate(([0.0], recall[:, j], [1.0]))\n",
        "            m_pre = numpy.concatenate(([1.0], precision[:, j], [0.0]))\n",
        "\n",
        "            # Compute the precision envelope\n",
        "            m_pre = numpy.flip(numpy.maximum.accumulate(numpy.flip(m_pre)))\n",
        "\n",
        "            # Integrate area under curve\n",
        "            x = numpy.linspace(0, 1, 101)  # 101-point interp (COCO)\n",
        "            ap[ci, j] = numpy.trapz(numpy.interp(x, m_rec, m_pre), x)  # integrate\n",
        "\n",
        "    # Compute F1 (harmonic mean of precision and recall)\n",
        "    f1 = 2 * p * r / (p + r + eps)\n",
        "\n",
        "    i = smooth(f1.mean(0), 0.1).argmax()  # max F1 index\n",
        "    p, r, f1 = p[:, i], r[:, i], f1[:, i]\n",
        "    tp = (r * nt).round()  # true positives\n",
        "    fp = (tp / (p + eps) - tp).round()  # false positives\n",
        "    ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
        "    m_pre, m_rec = p.mean(), r.mean()\n",
        "    map50, mean_ap = ap50.mean(), ap.mean()\n",
        "    return tp, fp, m_pre, m_rec, map50, mean_ap\n",
        "\n",
        "\n",
        "def strip_optimizer(filename):\n",
        "    x = torch.load(filename, map_location=torch.device('cpu'))\n",
        "    x['model'].half()  # to FP16\n",
        "    for p in x['model'].parameters():\n",
        "        p.requires_grad = False\n",
        "    torch.save(x, filename)\n",
        "\n",
        "\n",
        "def clip_gradients(model, max_norm=10.0):\n",
        "    parameters = model.parameters()\n",
        "    torch.nn.utils.clip_grad_norm_(parameters, max_norm=max_norm)\n",
        "\n",
        "\n",
        "class EMA:\n",
        "    \"\"\"\n",
        "    Updated Exponential Moving Average (EMA) from https://github.com/rwightman/pytorch-image-models\n",
        "    Keeps a moving average of everything in the model state_dict (parameters and buffers)\n",
        "    For EMA details see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, decay=0.9999, tau=2000, updates=0):\n",
        "        # Create EMA - keep in float32 to avoid reparameterization issues\n",
        "        self.ema = copy.deepcopy(model).eval().float()  # Always keep EMA in float32\n",
        "        self.updates = updates  # number of EMA updates\n",
        "        # decay exponential ramp (to help early epochs)\n",
        "        self.decay = lambda x: decay * (1 - math.exp(-x / tau))\n",
        "        for p in self.ema.parameters():\n",
        "            p.requires_grad_(False)\n",
        "\n",
        "    def update(self, model):\n",
        "        if hasattr(model, 'module'):\n",
        "            model = model.module\n",
        "        # Update EMA parameters\n",
        "        with torch.no_grad():\n",
        "            self.updates += 1\n",
        "            d = self.decay(self.updates)\n",
        "\n",
        "            msd = model.state_dict()  # model state_dict\n",
        "            for k, v in self.ema.state_dict().items():\n",
        "                if v.dtype.is_floating_point:\n",
        "                    v *= d\n",
        "                    v += (1 - d) * msd[k].detach().to(v.dtype)\n",
        "\n",
        "\n",
        "class AverageMeter:\n",
        "    def __init__(self):\n",
        "        self.num = 0\n",
        "        self.sum = 0\n",
        "        self.avg = 0\n",
        "\n",
        "    def update(self, v, n):\n",
        "        if not math.isnan(float(v)):\n",
        "            self.num = self.num + n\n",
        "            self.sum = self.sum + v * n\n",
        "            self.avg = self.sum / self.num\n",
        "\n",
        "\n",
        "class ComputeLoss:\n",
        "    def __init__(self, model, params):\n",
        "        super().__init__()\n",
        "        if hasattr(model, 'module'):\n",
        "            model = model.module\n",
        "\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "        # Get model parameters\n",
        "        self.multi_head = hasattr(model, 'head_p3')\n",
        "        self.num_classes = model.num_classes\n",
        "\n",
        "        # For single head, get output channels from head\n",
        "        if not self.multi_head:\n",
        "            m = model.head\n",
        "            self.nc = self.num_classes\n",
        "            # Output channels: 4 (bbox) + num_classes\n",
        "            self.no = 4 + self.nc\n",
        "        else:\n",
        "            # For multi-head, use p3 head as reference\n",
        "            m = model.head_p3\n",
        "            self.nc = self.num_classes\n",
        "            self.no = 4 + self.nc\n",
        "\n",
        "        self.bce = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
        "\n",
        "        # Define strides based on your model's feature map scales\n",
        "        if self.multi_head:\n",
        "            self.stride = torch.tensor([8, 16, 32], device=device)\n",
        "        else:\n",
        "            self.stride = torch.tensor([8], device=device)\n",
        "\n",
        "        self.device = device\n",
        "        self.params = params\n",
        "\n",
        "        # Task aligned assigner\n",
        "        self.top_k = 10\n",
        "        self.alpha = 0.5\n",
        "        self.beta = 6.0\n",
        "        self.eps = 1e-9\n",
        "\n",
        "        self.bs = 1\n",
        "        self.num_max_boxes = 0\n",
        "\n",
        "        # DFL Loss params\n",
        "        self.dfl_ch = 1  # Simplified DFL for now\n",
        "        self.project = torch.arange(self.dfl_ch, dtype=torch.float, device=device)\n",
        "\n",
        "    def __call__(self, outputs, targets):\n",
        "        # Handle different output formats\n",
        "        if self.multi_head:\n",
        "            # Multi-head outputs: [p3, p4, p5]\n",
        "            p3, p4, p5 = outputs\n",
        "            output_tensors = [p3, p4, p5]\n",
        "        else:\n",
        "            # Single-head output\n",
        "            output_tensors = [outputs]\n",
        "\n",
        "        # Get the first output for shape reference\n",
        "        x = output_tensors[0]\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # Concatenate outputs along spatial dimension for multi-head\n",
        "        if self.multi_head:\n",
        "            # For multi-head, we need to handle each head separately\n",
        "            all_outputs = []\n",
        "            for output in output_tensors:\n",
        "                # Flatten spatial dimensions: [B, C, H, W] -> [B, C, H*W]\n",
        "                flattened = output.view(batch_size, self.no, -1)\n",
        "                all_outputs.append(flattened)\n",
        "            # Concatenate along spatial dimension: [B, C, H1*W1 + H2*W2 + H3*W3]\n",
        "            output_cat = torch.cat(all_outputs, 2)\n",
        "        else:\n",
        "            # Single head: just flatten\n",
        "            output_cat = x.view(batch_size, self.no, -1)\n",
        "\n",
        "        # Split into box predictions and class scores\n",
        "        pred_output = output_cat[:, :4, :]  # [B, 4, N]\n",
        "        pred_scores = output_cat[:, 4:, :]  # [B, num_classes, N]\n",
        "\n",
        "        pred_output = pred_output.permute(0, 2, 1).contiguous()  # [B, N, 4]\n",
        "        pred_scores = pred_scores.permute(0, 2, 1).contiguous()  # [B, N, num_classes]\n",
        "\n",
        "        # Calculate size based on first output feature map\n",
        "        size = torch.tensor(x.shape[2:], dtype=pred_scores.dtype, device=self.device)\n",
        "        size = size * self.stride[0]\n",
        "\n",
        "        # Create anchors\n",
        "        anchor_points, stride_tensor = self.make_anchors(output_tensors, self.stride, 0.5)\n",
        "\n",
        "        # Process targets\n",
        "        if targets.shape[0] == 0:\n",
        "            gt = torch.zeros(pred_scores.shape[0], 0, 5, device=self.device)\n",
        "        else:\n",
        "            i = targets[:, 0]\n",
        "            _, counts = i.unique(return_counts=True)\n",
        "            gt = torch.zeros(pred_scores.shape[0], counts.max(), 5, device=self.device)\n",
        "            for j in range(pred_scores.shape[0]):\n",
        "                matches = i == j\n",
        "                n = matches.sum()\n",
        "                if n:\n",
        "                    gt[j, :n] = targets[matches, 1:]\n",
        "            # Convert from normalized to pixel coordinates\n",
        "            from data_loader import wh2xy\n",
        "            gt[..., 1:5] = wh2xy(gt[..., 1:5].mul_(size[[1, 0, 1, 0]]))\n",
        "\n",
        "        gt_labels, gt_bboxes = gt.split((1, 4), 2)\n",
        "        mask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0)\n",
        "\n",
        "        # Process predictions to bounding boxes\n",
        "        b, a, c = pred_output.shape\n",
        "        # Simple box decoding (replace DFL with direct regression for now)\n",
        "        pred_bboxes = torch.sigmoid(pred_output)  # Simple activation for box coordinates\n",
        "\n",
        "        # Convert from center+wh to xyxy format\n",
        "        a, b_vals = torch.split(pred_bboxes, 2, -1)\n",
        "        pred_bboxes_xyxy = torch.cat((anchor_points - a, anchor_points + b_vals), -1)\n",
        "\n",
        "        scores = pred_scores.detach().sigmoid()\n",
        "        bboxes = (pred_bboxes_xyxy.detach() * stride_tensor).type(gt_bboxes.dtype)\n",
        "\n",
        "        # Task-aligned assignment\n",
        "        target_bboxes, target_scores, fg_mask = self.assign(scores, bboxes,\n",
        "                                                          gt_labels, gt_bboxes, mask_gt,\n",
        "                                                          anchor_points * stride_tensor)\n",
        "\n",
        "        target_bboxes /= stride_tensor\n",
        "        target_scores_sum = max(target_scores.sum(), 1)\n",
        "\n",
        "        # cls loss\n",
        "        loss_cls = self.bce(pred_scores, target_scores.to(pred_scores.dtype))\n",
        "        loss_cls = loss_cls.sum() / target_scores_sum\n",
        "\n",
        "        # box loss\n",
        "        loss_box = torch.zeros(1, device=self.device)\n",
        "        loss_dfl = torch.zeros(1, device=self.device)\n",
        "\n",
        "        if fg_mask.sum():\n",
        "            weight = torch.masked_select(target_scores.sum(-1), fg_mask).unsqueeze(-1)\n",
        "            loss_box = self.iou(pred_bboxes_xyxy[fg_mask], target_bboxes[fg_mask])\n",
        "            loss_box = ((1.0 - loss_box) * weight).sum() / target_scores_sum\n",
        "\n",
        "            # Simplified DFL loss\n",
        "            a, b_vals = torch.split(target_bboxes, 2, -1)\n",
        "            target_lt_rb = torch.cat((anchor_points - a, b_vals - anchor_points), -1)\n",
        "            target_lt_rb = target_lt_rb.clamp(0, self.dfl_ch - 1.01)\n",
        "            loss_dfl = self.df_loss(pred_output[fg_mask].view(-1, 4), target_lt_rb[fg_mask])\n",
        "            loss_dfl = (loss_dfl * weight).sum() / target_scores_sum\n",
        "\n",
        "        loss_cls *= self.params['cls']\n",
        "        loss_box *= self.params['box']\n",
        "        loss_dfl *= self.params['dfl']\n",
        "        return loss_cls + loss_box + loss_dfl\n",
        "\n",
        "    @staticmethod\n",
        "    def make_anchors(x, strides, offset=0.5):\n",
        "        \"\"\"Generate anchors from features\"\"\"\n",
        "        assert x is not None\n",
        "        anchor_points, stride_tensor = [], []\n",
        "        for i, stride in enumerate(strides):\n",
        "            _, _, h, w = x[i].shape\n",
        "            sx = torch.arange(end=w, dtype=x[i].dtype, device=x[i].device) + offset\n",
        "            sy = torch.arange(end=h, dtype=x[i].dtype, device=x[i].device) + offset\n",
        "            sy, sx = torch.meshgrid(sy, sx, indexing='ij')\n",
        "            anchor_points.append(torch.stack((sx, sy), -1).view(-1, 2))\n",
        "            stride_tensor.append(torch.full((h * w, 1), stride, dtype=x[i].dtype, device=x[i].device))\n",
        "        return torch.cat(anchor_points), torch.cat(stride_tensor)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def assign(self, pred_scores, pred_bboxes, true_labels, true_bboxes, true_mask, anchors):\n",
        "        \"\"\"Task-aligned assignment\"\"\"\n",
        "        self.bs = pred_scores.size(0)\n",
        "        self.num_max_boxes = true_bboxes.size(1)\n",
        "\n",
        "        if self.num_max_boxes == 0:\n",
        "            device = true_bboxes.device\n",
        "            return (torch.zeros_like(pred_bboxes).to(device),\n",
        "                    torch.zeros_like(pred_scores).to(device),\n",
        "                    torch.zeros_like(pred_scores[..., 0]).to(device))\n",
        "\n",
        "        # Simplified assignment for now\n",
        "        i = torch.zeros([2, self.bs, self.num_max_boxes], dtype=torch.long)\n",
        "        i[0] = torch.arange(end=self.bs).view(-1, 1).repeat(1, self.num_max_boxes)\n",
        "        i[1] = true_labels.long().squeeze(-1)\n",
        "\n",
        "        overlaps = self.iou(true_bboxes.unsqueeze(2), pred_bboxes.unsqueeze(1))\n",
        "        overlaps = overlaps.squeeze(3).clamp(0)\n",
        "\n",
        "        # Simple assignment based on IoU\n",
        "        max_overlaps, max_indices = overlaps.max(2)\n",
        "        fg_mask = max_overlaps > 0.5\n",
        "\n",
        "        target_bboxes = torch.zeros_like(pred_bboxes)\n",
        "        target_scores = torch.zeros_like(pred_scores)\n",
        "\n",
        "        for b in range(self.bs):\n",
        "            for j in range(self.num_max_boxes):\n",
        "                if true_mask[b, j] and fg_mask[b, j]:\n",
        "                    idx = max_indices[b, j]\n",
        "                    target_bboxes[b, idx] = true_bboxes[b, j]\n",
        "                    target_scores[b, idx, true_labels[b, j].long()] = 1.0\n",
        "\n",
        "        return target_bboxes, target_scores, fg_mask\n",
        "\n",
        "    @staticmethod\n",
        "    def df_loss(pred_dist, target):\n",
        "        \"\"\"Simplified distribution focal loss\"\"\"\n",
        "        # For now, use smooth L1 loss as placeholder\n",
        "        return F.smooth_l1_loss(pred_dist, target, reduction='none')\n",
        "\n",
        "    @staticmethod\n",
        "    def iou(box1, box2, eps=1e-7):\n",
        "        \"\"\"Calculate IoU between boxes\"\"\"\n",
        "        # Get the coordinates of bounding boxes\n",
        "        b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)\n",
        "        b2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, -1)\n",
        "\n",
        "        # Intersection area\n",
        "        inter_x1 = torch.max(b1_x1, b2_x1)\n",
        "        inter_y1 = torch.max(b1_y1, b2_y1)\n",
        "        inter_x2 = torch.min(b1_x2, b2_x2)\n",
        "        inter_y2 = torch.min(b1_y2, b2_y2)\n",
        "\n",
        "        inter_area = (inter_x2 - inter_x1).clamp(0) * (inter_y2 - inter_y1).clamp(0)\n",
        "\n",
        "        # Union Area\n",
        "        b1_area = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)\n",
        "        b2_area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)\n",
        "\n",
        "        union_area = b1_area + b2_area - inter_area + eps\n",
        "\n",
        "        # IoU\n",
        "        iou = inter_area / union_area\n",
        "\n",
        "        return iou\n"
      ],
      "metadata": {
        "id": "EqnkezTpjydw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_loader.py\n",
        "\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Configuration\n",
        "DATASET_CONFIG = {\n",
        "    'input_size': 640,\n",
        "    'batch_size': 16,\n",
        "    'num_workers': 1,\n",
        "    'train_test_split': 0.2,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "TRAIN_AUGMENTATION = {\n",
        "    'mosaic': 1.0,\n",
        "    'mix_up': 0.1,\n",
        "    'hsv_h': 0.015,\n",
        "    'hsv_s': 0.7,\n",
        "    'hsv_v': 0.4,\n",
        "    'degrees': 0.0,\n",
        "    'translate': 0.1,\n",
        "    'scale': 0.5,\n",
        "    'shear': 0.0,\n",
        "    'flip_lr': 0.5,\n",
        "    'flip_ud': 0.0,\n",
        "}\n",
        "\n",
        "VAL_AUGMENTATION = {\n",
        "    'mosaic': 0.0,\n",
        "    'mix_up': 0.0,\n",
        "    'hsv_h': 0.0,\n",
        "    'hsv_s': 0.0,\n",
        "    'hsv_v': 0.0,\n",
        "    'degrees': 0.0,\n",
        "    'translate': 0.0,\n",
        "    'scale': 0.0,\n",
        "    'shear': 0.0,\n",
        "    'flip_lr': 0.0,\n",
        "    'flip_ud': 0.0,\n",
        "}\n",
        "\n",
        "FORMATS = 'bmp', 'dng', 'jpeg', 'jpg', 'mpo', 'png', 'tif', 'tiff', 'webp'\n",
        "\n",
        "# Helper functions (keep all the same as before)\n",
        "def wh2xy(x, w=640, h=640, pad_w=0, pad_h=0):\n",
        "    y = np.copy(x)\n",
        "    y[:, 0] = w * (x[:, 0] - x[:, 2] / 2) + pad_w\n",
        "    y[:, 1] = h * (x[:, 1] - x[:, 3] / 2) + pad_h\n",
        "    y[:, 2] = w * (x[:, 0] + x[:, 2] / 2) + pad_w\n",
        "    y[:, 3] = h * (x[:, 1] + x[:, 3] / 2) + pad_h\n",
        "    return y\n",
        "\n",
        "def xy2wh(x, w=640, h=640):\n",
        "    x[:, [0, 2]] = x[:, [0, 2]].clip(0, w - 1E-3)\n",
        "    x[:, [1, 3]] = x[:, [1, 3]].clip(0, h - 1E-3)\n",
        "    y = np.copy(x)\n",
        "    y[:, 0] = ((x[:, 0] + x[:, 2]) / 2) / w\n",
        "    y[:, 1] = ((x[:, 1] + x[:, 3]) / 2) / h\n",
        "    y[:, 2] = (x[:, 2] - x[:, 0]) / w\n",
        "    y[:, 3] = (x[:, 3] - x[:, 1]) / h\n",
        "    return y\n",
        "\n",
        "def resample():\n",
        "    choices = (cv2.INTER_AREA, cv2.INTER_CUBIC, cv2.INTER_LINEAR, cv2.INTER_NEAREST, cv2.INTER_LANCZOS4)\n",
        "    return random.choice(choices)\n",
        "\n",
        "def augment_hsv(image, params):\n",
        "    h = params['hsv_h']\n",
        "    s = params['hsv_s']\n",
        "    v = params['hsv_v']\n",
        "    r = np.random.uniform(-1, 1, 3) * [h, s, v] + 1\n",
        "    h, s, v = cv2.split(cv2.cvtColor(image, cv2.COLOR_BGR2HSV))\n",
        "    x = np.arange(0, 256, dtype=r.dtype)\n",
        "    lut_h = ((x * r[0]) % 180).astype('uint8')\n",
        "    lut_s = np.clip(x * r[1], 0, 255).astype('uint8')\n",
        "    lut_v = np.clip(x * r[2], 0, 255).astype('uint8')\n",
        "    im_hsv = cv2.merge((cv2.LUT(h, lut_h), cv2.LUT(s, lut_s), cv2.LUT(v, lut_v)))\n",
        "    cv2.cvtColor(im_hsv, cv2.COLOR_HSV2BGR, dst=image)\n",
        "\n",
        "def resize(image, input_size, augment):\n",
        "    shape = image.shape[:2]\n",
        "    r = min(input_size / shape[0], input_size / shape[1])\n",
        "    if not augment:\n",
        "        r = min(r, 1.0)\n",
        "    pad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    w = (input_size - pad[0]) / 2\n",
        "    h = (input_size - pad[1]) / 2\n",
        "    if shape[::-1] != pad:\n",
        "        image = cv2.resize(image, dsize=pad, interpolation=resample() if augment else cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(h - 0.1)), int(round(h + 0.1))\n",
        "    left, right = int(round(w - 0.1)), int(round(w + 0.1))\n",
        "    image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT)\n",
        "    return image, (r, r), (w, h)\n",
        "\n",
        "def candidates(box1, box2):\n",
        "    w1, h1 = box1[2] - box1[0], box1[3] - box1[1]\n",
        "    w2, h2 = box2[2] - box2[0], box2[3] - box2[1]\n",
        "    aspect_ratio = np.maximum(w2 / (h2 + 1e-16), h2 / (w2 + 1e-16))\n",
        "    return (w2 > 2) & (h2 > 2) & (w2 * h2 / (w1 * h1 + 1e-16) > 0.1) & (aspect_ratio < 100)\n",
        "\n",
        "def random_perspective(samples, targets, params, border=(0, 0)):\n",
        "    h = samples.shape[0] + border[0] * 2\n",
        "    w = samples.shape[1] + border[1] * 2\n",
        "    center = np.eye(3)\n",
        "    center[0, 2] = -samples.shape[1] / 2\n",
        "    center[1, 2] = -samples.shape[0] / 2\n",
        "    perspective = np.eye(3)\n",
        "    rotate = np.eye(3)\n",
        "    a = random.uniform(-params['degrees'], params['degrees'])\n",
        "    s = random.uniform(1 - params['scale'], 1 + params['scale'])\n",
        "    rotate[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)\n",
        "    shear = np.eye(3)\n",
        "    shear[0, 1] = math.tan(random.uniform(-params['shear'], params['shear']) * math.pi / 180)\n",
        "    shear[1, 0] = math.tan(random.uniform(-params['shear'], params['shear']) * math.pi / 180)\n",
        "    translate = np.eye(3)\n",
        "    translate[0, 2] = random.uniform(0.5 - params['translate'], 0.5 + params['translate']) * w\n",
        "    translate[1, 2] = random.uniform(0.5 - params['translate'], 0.5 + params['translate']) * h\n",
        "    matrix = translate @ shear @ rotate @ perspective @ center\n",
        "    if (border[0] != 0) or (border[1] != 0) or (matrix != np.eye(3)).any():\n",
        "        samples = cv2.warpAffine(samples, matrix[:2], dsize=(w, h), borderValue=(0, 0, 0))\n",
        "    n = len(targets)\n",
        "    if n:\n",
        "        xy = np.ones((n * 4, 3))\n",
        "        xy[:, :2] = targets[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(n * 4, 2)\n",
        "        xy = xy @ matrix.T\n",
        "        xy = xy[:, :2].reshape(n, 8)\n",
        "        x = xy[:, [0, 2, 4, 6]]\n",
        "        y = xy[:, [1, 3, 5, 7]]\n",
        "        new = np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n",
        "        new[:, [0, 2]] = new[:, [0, 2]].clip(0, w)\n",
        "        new[:, [1, 3]] = new[:, [1, 3]].clip(0, h)\n",
        "        indices = candidates(box1=targets[:, 1:5].T * s, box2=new.T)\n",
        "        targets = targets[indices]\n",
        "        targets[:, 1:5] = new[indices]\n",
        "    return samples, targets\n",
        "\n",
        "def mix_up(image1, label1, image2, label2):\n",
        "    alpha = np.random.beta(32.0, 32.0)\n",
        "    image = (image1 * alpha + image2 * (1 - alpha)).astype(np.uint8)\n",
        "    label = np.concatenate((label1, label2), 0)\n",
        "    return image, label\n",
        "\n",
        "class Albumentations:\n",
        "    def __init__(self):\n",
        "        self.transform = None\n",
        "        try:\n",
        "            import albumentations as album\n",
        "            transforms = [album.Blur(p=0.01), album.CLAHE(p=0.01),\n",
        "                         album.ToGray(p=0.01), album.MedianBlur(p=0.01)]\n",
        "            self.transform = album.Compose(transforms, album.BboxParams('yolo', ['class_labels']))\n",
        "        except ImportError:\n",
        "            pass\n",
        "\n",
        "    def __call__(self, image, label):\n",
        "        if self.transform and len(label) > 0:\n",
        "            x = self.transform(image=image, bboxes=label[:, 1:], class_labels=label[:, 0])\n",
        "            image = x['image']\n",
        "            label = np.array([[c, *b] for c, b in zip(x['class_labels'], x['bboxes'])])\n",
        "        return image, label\n",
        "\n",
        "class CarDetectionDataset(data.Dataset):\n",
        "    def __init__(self, filenames, input_size, params, augment):\n",
        "        self.params = params\n",
        "        self.mosaic = augment\n",
        "        self.augment = augment\n",
        "        self.input_size = input_size\n",
        "\n",
        "        # Load labels without cache in read-only directories\n",
        "        cache = self.load_label(filenames)\n",
        "        labels, shapes = zip(*cache.values())\n",
        "        self.labels = list(labels)\n",
        "        self.shapes = np.array(shapes, dtype=np.float64)\n",
        "        self.filenames = list(cache.keys())\n",
        "        self.n = len(shapes)\n",
        "        self.indices = range(self.n)\n",
        "        self.albumentations = Albumentations()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = self.indices[index]\n",
        "        params = self.params\n",
        "        mosaic = self.mosaic and random.random() < params['mosaic']\n",
        "\n",
        "        if mosaic:\n",
        "            # Load MOSAIC\n",
        "            image, label = self.load_mosaic(index, params)\n",
        "            shapes = None  # Mosaic doesn't have original shapes\n",
        "\n",
        "            # MixUp augmentation\n",
        "            if random.random() < params['mix_up']:\n",
        "                index = random.choice(self.indices)\n",
        "                mix_image1, mix_label1 = image, label\n",
        "                mix_image2, mix_label2 = self.load_mosaic(index, params)\n",
        "                image, label = mix_up(mix_image1, mix_label1, mix_image2, mix_label2)\n",
        "        else:\n",
        "            # Load image\n",
        "            image, shape = self.load_image(index)\n",
        "            h, w = image.shape[:2]\n",
        "\n",
        "            # Resize\n",
        "            image, ratio, pad = resize(image, self.input_size, self.augment)\n",
        "            shapes = shape, ((h / shape[0], w / shape[1]), pad)  # FIXED: define shapes here\n",
        "\n",
        "            label = self.labels[index].copy()\n",
        "            if label.size:\n",
        "                label[:, 1:] = wh2xy(label[:, 1:], ratio[0] * w, ratio[1] * h, pad[0], pad[1])\n",
        "            if self.augment:\n",
        "                image, label = random_perspective(image, label, params)\n",
        "\n",
        "        nl = len(label)\n",
        "        if nl:\n",
        "            label[:, 1:5] = xy2wh(label[:, 1:5], image.shape[1], image.shape[0])\n",
        "\n",
        "        if self.augment:\n",
        "            image, label = self.albumentations(image, label)\n",
        "            nl = len(label)\n",
        "            augment_hsv(image, self.params)\n",
        "            if random.random() < params['flip_ud']:\n",
        "                image = np.flipud(image)\n",
        "                if nl:\n",
        "                    label[:, 2] = 1 - label[:, 2]\n",
        "            if random.random() < params['flip_lr']:\n",
        "                image = np.fliplr(image)\n",
        "                if nl:\n",
        "                    label[:, 1] = 1 - label[:, 1]\n",
        "\n",
        "        target = torch.zeros((nl, 6))\n",
        "        if nl:\n",
        "            target[:, 1:] = torch.from_numpy(label)\n",
        "\n",
        "        sample = image.transpose((2, 0, 1))[::-1]\n",
        "        sample = np.ascontiguousarray(sample)\n",
        "\n",
        "        return torch.from_numpy(sample), target, shapes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def load_image(self, i):\n",
        "        image = cv2.imread(self.filenames[i])\n",
        "        if image is None:\n",
        "            raise ValueError(f\"Could not load image: {self.filenames[i]}\")\n",
        "        h, w = image.shape[:2]\n",
        "        r = self.input_size / max(h, w)\n",
        "        if r != 1:\n",
        "            image = cv2.resize(image,\n",
        "                             dsize=(int(w * r), int(h * r)),\n",
        "                             interpolation=resample() if self.augment else cv2.INTER_LINEAR)\n",
        "        return image, (h, w)\n",
        "\n",
        "    def load_mosaic(self, index, params):\n",
        "        label4 = []\n",
        "        image4 = np.full((self.input_size * 2, self.input_size * 2, 3), 0, dtype=np.uint8)\n",
        "        border = [-self.input_size // 2, -self.input_size // 2]\n",
        "        xc = int(random.uniform(-border[0], 2 * self.input_size + border[1]))\n",
        "        yc = int(random.uniform(-border[0], 2 * self.input_size + border[1]))\n",
        "        indices = [index] + random.choices(self.indices, k=3)\n",
        "        random.shuffle(indices)\n",
        "\n",
        "        for i, index in enumerate(indices):\n",
        "            image, _ = self.load_image(index)\n",
        "            shape = image.shape\n",
        "            if i == 0:\n",
        "                x1a, y1a = max(xc - shape[1], 0), max(yc - shape[0], 0)\n",
        "                x2a, y2a = xc, yc\n",
        "                x1b, y1b = shape[1] - (x2a - x1a), shape[0] - (y2a - y1a)\n",
        "                x2b, y2b = shape[1], shape[0]\n",
        "            elif i == 1:\n",
        "                x1a, y1a = xc, max(yc - shape[0], 0)\n",
        "                x2a, y2a = min(xc + shape[1], self.input_size * 2), yc\n",
        "                x1b, y1b = 0, shape[0] - (y2a - y1a)\n",
        "                x2b, y2b = min(shape[1], x2a - x1a), shape[0]\n",
        "            elif i == 2:\n",
        "                x1a, y1a = max(xc - shape[1], 0), yc\n",
        "                x2a, y2a = xc, min(self.input_size * 2, yc + shape[0])\n",
        "                x1b, y1b = shape[1] - (x2a - x1a), 0\n",
        "                x2b, y2b = shape[1], min(y2a - y1a, shape[0])\n",
        "            elif i == 3:\n",
        "                x1a, y1a = xc, yc\n",
        "                x2a, y2a = min(xc + shape[1], self.input_size * 2), min(self.input_size * 2, yc + shape[0])\n",
        "                x1b, y1b = 0, 0\n",
        "                x2b, y2b = min(shape[1], x2a - x1a), min(y2a - y1a, shape[0])\n",
        "\n",
        "            image4[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n",
        "            pad_w, pad_h = x1a - x1b, y1a - y1b\n",
        "            label = self.labels[index].copy()\n",
        "            if len(label):\n",
        "                label[:, 1:] = wh2xy(label[:, 1:], shape[1], shape[0], pad_w, pad_h)\n",
        "            label4.append(label)\n",
        "\n",
        "        label4 = np.concatenate(label4, 0)\n",
        "        for x in label4[:, 1:]:\n",
        "            np.clip(x, 0, 2 * self.input_size, out=x)\n",
        "        image4, label4 = random_perspective(image4, label4, params, border)\n",
        "        return image4, label4\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        samples, targets, shapes = zip(*batch)\n",
        "        for i, item in enumerate(targets):\n",
        "            item[:, 0] = i\n",
        "        return torch.stack(samples, 0), torch.cat(targets, 0), shapes\n",
        "\n",
        "    @staticmethod\n",
        "    def load_label(filenames):\n",
        "        \"\"\"Load labels without using cache in read-only directories\"\"\"\n",
        "        # Use current directory for cache instead of dataset directory\n",
        "        cache_dir = './dataset_cache'\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "        # Create a unique cache filename based on the first image directory\n",
        "        first_file_dir = os.path.dirname(filenames[0])\n",
        "        cache_name = os.path.basename(first_file_dir) + '.cache'\n",
        "        cache_path = os.path.join(cache_dir, cache_name)\n",
        "\n",
        "        # Try to load cache if it exists\n",
        "        if os.path.exists(cache_path):\n",
        "            try:\n",
        "                print(f\"Loading cache from: {cache_path}\")\n",
        "                # Load with weights_only=False to handle numpy arrays\n",
        "                return torch.load(cache_path, weights_only=False)\n",
        "            except Exception as e:\n",
        "                print(f\"Cache loading failed: {e}, regenerating...\")\n",
        "\n",
        "        x = {}\n",
        "        valid_count = 0\n",
        "        for filename in filenames:\n",
        "            try:\n",
        "                with open(filename, 'rb') as f:\n",
        "                    image = Image.open(f)\n",
        "                    image.verify()\n",
        "                shape = image.size\n",
        "\n",
        "                if not ((shape[0] > 9) & (shape[1] > 9)):\n",
        "                    print(f\"Warning: image size {shape} <10 pixels in {filename}\")\n",
        "                    continue\n",
        "\n",
        "                if image.format.lower() not in FORMATS:\n",
        "                    print(f\"Warning: invalid image format {image.format} in {filename}\")\n",
        "                    continue\n",
        "\n",
        "                # Find label file - handle different dataset structures\n",
        "                label_path = None\n",
        "                possible_paths = [\n",
        "                    filename.replace('images', 'labels').rsplit('.', 1)[0] + '.txt',\n",
        "                    filename.replace('testing_images', 'testing_labels').rsplit('.', 1)[0] + '.txt',\n",
        "                    filename.replace('training_images', 'training_labels').rsplit('.', 1)[0] + '.txt',\n",
        "                ]\n",
        "\n",
        "                for path in possible_paths:\n",
        "                    if os.path.isfile(path):\n",
        "                        label_path = path\n",
        "                        break\n",
        "\n",
        "                if label_path and os.path.isfile(label_path):\n",
        "                    with open(label_path) as f:\n",
        "                        label_lines = [x.split() for x in f.read().strip().splitlines() if len(x)]\n",
        "                        label = np.array(label_lines, dtype=np.float32)\n",
        "                    nl = len(label)\n",
        "                    if nl:\n",
        "                        if label.shape[1] != 5:\n",
        "                            print(f\"Warning: labels require 5 columns in {label_path}\")\n",
        "                            continue\n",
        "                        if not (label >= 0).all():\n",
        "                            print(f\"Warning: negative label values in {label_path}\")\n",
        "                            continue\n",
        "                        if not (label[:, 1:] <= 1).all():\n",
        "                            print(f\"Warning: non-normalized coordinates in {label_path}\")\n",
        "                            continue\n",
        "                        _, i = np.unique(label, axis=0, return_index=True)\n",
        "                        if len(i) < nl:\n",
        "                            label = label[i]\n",
        "                    else:\n",
        "                        label = np.zeros((0, 5), dtype=np.float32)\n",
        "                else:\n",
        "                    # No label file found\n",
        "                    label = np.zeros((0, 5), dtype=np.float32)\n",
        "                    print(f\"Warning: No label file found for {filename}\")\n",
        "\n",
        "                x[filename] = [label, shape]\n",
        "                valid_count += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {filename}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"Successfully loaded {valid_count}/{len(filenames)} images\")\n",
        "\n",
        "        # Save cache if we have valid data\n",
        "        if valid_count > 0:\n",
        "            try:\n",
        "                torch.save(x, cache_path)\n",
        "                print(f\"Cache saved to: {cache_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not save cache: {e}\")\n",
        "\n",
        "        return x\n",
        "\n",
        "def find_image_files(dataset_path):\n",
        "    \"\"\"Find all image files in the dataset\"\"\"\n",
        "    image_files = []\n",
        "    for root, dirs, files in os.walk(dataset_path):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
        "                image_files.append(os.path.join(root, file))\n",
        "    return image_files\n",
        "\n",
        "def create_data_loaders(dataset_path):\n",
        "    \"\"\"Create training and validation data loaders\"\"\"\n",
        "    image_files = find_image_files(dataset_path)\n",
        "    print(f\"Found {len(image_files)} images\")\n",
        "\n",
        "    if len(image_files) == 0:\n",
        "        raise ValueError(\"No images found in the dataset path!\")\n",
        "\n",
        "    # Split dataset\n",
        "    train_files, val_files = train_test_split(\n",
        "        image_files,\n",
        "        test_size=DATASET_CONFIG['train_test_split'],\n",
        "        random_state=DATASET_CONFIG['random_state']\n",
        "    )\n",
        "\n",
        "    print(f\"Creating training dataset with {len(train_files)} images...\")\n",
        "    train_dataset = CarDetectionDataset(\n",
        "        train_files,\n",
        "        DATASET_CONFIG['input_size'],\n",
        "        TRAIN_AUGMENTATION,\n",
        "        augment=True\n",
        "    )\n",
        "\n",
        "    print(f\"Creating validation dataset with {len(val_files)} images...\")\n",
        "    val_dataset = CarDetectionDataset(\n",
        "        val_files,\n",
        "        DATASET_CONFIG['input_size'],\n",
        "        VAL_AUGMENTATION,\n",
        "        augment=False\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=DATASET_CONFIG['batch_size'],\n",
        "        shuffle=True,\n",
        "        num_workers=0,  # Set to 0 to avoid multiprocessing issues\n",
        "        pin_memory=False,  # Set to False since we're not using GPU for data loading\n",
        "        collate_fn=CarDetectionDataset.collate_fn\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=DATASET_CONFIG['batch_size'],\n",
        "        shuffle=False,\n",
        "        num_workers=0,  # Set to 0 to avoid multiprocessing issues\n",
        "        pin_memory=False,  # Set to False since we're not using GPU for data loading\n",
        "        collate_fn=CarDetectionDataset.collate_fn\n",
        "    )\n",
        "\n",
        "    print(f\"Training samples: {len(train_files)}\")\n",
        "    print(f\"Validation samples: {len(val_files)}\")\n",
        "    print(f\"Train batches: {len(train_loader)}\")\n",
        "    print(f\"Val batches: {len(val_loader)}\")\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "gyJG8hLYlaXT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install albumentations kagglehub\n",
        "import kagglehub\n",
        "\n",
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\"sshikamaru/car-object-detection\")\n",
        "print(f\"Dataset path: {path}\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader, val_loader = create_data_loaders(path)\n",
        "\n",
        "# Test the data loader\n",
        "for images, targets, shapes in train_loader:\n",
        "    print(f\"Batch - Images: {images.shape}, Targets: {targets.shape}\")\n",
        "    break"
      ],
      "metadata": {
        "id": "3ZlIdijNwj0h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c691dad5-63d1-4a7a-8eb3-69a29ad9d1ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.12/dist-packages (2.0.8)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from albumentations) (1.16.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from albumentations) (6.0.3)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.12.3)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.12/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.12/dist-packages (from albumentations) (4.12.0.88)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (4.2.3)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations) (6.5.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2025.11.12)\n",
            "Using Colab cache for faster access to the 'car-object-detection' dataset.\n",
            "Dataset path: /kaggle/input/car-object-detection\n",
            "Found 1176 images\n",
            "Creating training dataset with 940 images...\n",
            "Loading cache from: ./dataset_cache/testing_images.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/albumentations/core/composition.py:331: UserWarning: Got processor for bboxes, but no transform to process it.\n",
            "  self._set_keys()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating validation dataset with 236 images...\n",
            "Loading cache from: ./dataset_cache/testing_images.cache\n",
            "Training samples: 940\n",
            "Validation samples: 236\n",
            "Train batches: 59\n",
            "Val batches: 59\n",
            "Batch - Images: torch.Size([16, 3, 640, 640]), Targets: torch.Size([0, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import csv\n",
        "import os\n",
        "import warnings\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import tqdm\n",
        "import yaml\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "FORMATS = ('bmp','dng','jpeg','jpg','mpo','png','tif','tiff','webp')\n",
        "\n",
        "def _list_images(root):\n",
        "    root = Path(root)\n",
        "    out = []\n",
        "    for ext in FORMATS:\n",
        "        out += [str(p) for p in root.rglob(f'*.{ext}')]\n",
        "        out += [str(p) for p in root.rglob(f'*.{ext.upper()}')]\n",
        "    return out\n",
        "\n",
        "class YoloTrainer:\n",
        "    def __init__(self, args, params):\n",
        "        self.args = args\n",
        "        self.params = params\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(\"Device:\", self.device)\n",
        "\n",
        "        # Model configuration\n",
        "        num_classes = params.get('nc', 5)  # Default to 1 class (car)\n",
        "        multi_head = params.get('multi_head', False)\n",
        "\n",
        "        self.model = YOLOv8Hybrid(num_classes=num_classes, multi_head=multi_head).to(self.device)\n",
        "        self.world_size = 1\n",
        "        self.accumulate = max(round(64 / args.batch_size), 1)\n",
        "        self.params['weight_decay'] = float(self.params.get('weight_decay', 0.0005))\n",
        "\n",
        "        self.optimizer = self._setup_optimizer()\n",
        "        for g in self.optimizer.param_groups:\n",
        "            g.setdefault('initial_lr', g.get('lr', self.params.get('lr0', 0.01)))\n",
        "\n",
        "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
        "            self.optimizer, lr_lambda=self._get_lr_lambda()\n",
        "        )\n",
        "\n",
        "        self.ema = EMA(self.model)\n",
        "\n",
        "        # Loss parameters\n",
        "        loss_params = {\n",
        "            'cls': params.get('cls_loss', 0.5),\n",
        "            'box': params.get('box_loss', 7.5),\n",
        "            'dfl': params.get('dfl_loss', 1.5)\n",
        "        }\n",
        "        self.criterion = ComputeLoss(self.model, loss_params)\n",
        "\n",
        "        self.scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
        "        self.best_map = 0.0\n",
        "        os.makedirs('weights', exist_ok=True)\n",
        "\n",
        "    def _get_lr_lambda(self):\n",
        "        def lr_lambda(epoch):\n",
        "            return (1 - epoch / float(self.args.epochs)) * \\\n",
        "                   (1.0 - float(self.params.get('lrf', 0.01))) + \\\n",
        "                   float(self.params.get('lrf', 0.01))\n",
        "        return lr_lambda\n",
        "\n",
        "    def _setup_optimizer(self):\n",
        "        biases, bn_weights, weights = [], [], []\n",
        "        for module in self.model.modules():\n",
        "            if hasattr(module, 'bias') and isinstance(getattr(module, 'bias'), nn.Parameter):\n",
        "                biases.append(module.bias)\n",
        "            if isinstance(module, (nn.BatchNorm2d, nn.SyncBatchNorm)):\n",
        "                if hasattr(module, 'weight'):\n",
        "                    bn_weights.append(module.weight)\n",
        "            elif hasattr(module, 'weight') and isinstance(getattr(module, 'weight'), nn.Parameter):\n",
        "                weights.append(module.weight)\n",
        "\n",
        "        lr0 = float(self.params.get('lr0', 0.01))\n",
        "        momentum = float(self.params.get('momentum', 0.937))\n",
        "        weight_decay = float(self.params.get('weight_decay', 0.0005))\n",
        "\n",
        "        optimizer = torch.optim.SGD(biases, lr=lr0, momentum=momentum, nesterov=True)\n",
        "        optimizer.add_param_group({'params': weights, 'weight_decay': weight_decay, 'lr': lr0})\n",
        "        optimizer.add_param_group({'params': bn_weights, 'weight_decay': 0.0, 'lr': lr0})\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def _load_dataset(self, train=True):\n",
        "        \"\"\"Load dataset using the CarDetectionDataset class\"\"\"\n",
        "        data_cfg = self.params.get('data', {})\n",
        "\n",
        "        if train:\n",
        "            # For training, use the dataset path directly\n",
        "            dataset_path = data_cfg.get('path', '/kaggle/input/car-object-detection')\n",
        "            print(f\"Loading {'training' if train else 'validation'} dataset from: {dataset_path}\")\n",
        "\n",
        "            # Use the create_data_loaders function from data_loader\n",
        "            train_loader, val_loader = create_data_loaders(dataset_path)\n",
        "            return train_loader if train else val_loader\n",
        "        else:\n",
        "            # For validation, return the validation loader\n",
        "            dataset_path = data_cfg.get('path', '/kaggle/input/car-object-detection')\n",
        "            train_loader, val_loader = create_data_loaders(dataset_path)\n",
        "            return val_loader\n",
        "\n",
        "    def train_epoch(self, loader, epoch, warmup_iters, num_batches):\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        running_loss = 0.0\n",
        "        pbar = tqdm.tqdm(loader, desc=f\"Epoch {epoch+1}/{self.args.epochs}\")\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        for i, (images, targets, _) in enumerate(pbar):\n",
        "            iteration = i + epoch * num_batches\n",
        "            images = images.to(self.device).float() / 255.0\n",
        "            targets = targets.to(self.device)\n",
        "\n",
        "            # Learning rate warmup\n",
        "            if iteration < warmup_iters:\n",
        "                lr_scale = np.interp(iteration, [0, warmup_iters], [0.2, 1.0])\n",
        "                for g in self.optimizer.param_groups:\n",
        "                    g['lr'] = g['initial_lr'] * lr_scale\n",
        "\n",
        "            # Mixed precision training\n",
        "            if torch.cuda.is_available():\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = self.model(images)\n",
        "                    loss = self.criterion(outputs, targets)\n",
        "\n",
        "                self.scaler.scale(loss).backward()\n",
        "\n",
        "                # Gradient accumulation\n",
        "                if (i + 1) % self.accumulate == 0:\n",
        "                    self.scaler.step(self.optimizer)\n",
        "                    self.scaler.update()\n",
        "                    self.optimizer.zero_grad()\n",
        "            else:\n",
        "                # CPU training\n",
        "                outputs = self.model(images)\n",
        "                loss = self.criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient accumulation\n",
        "                if (i + 1) % self.accumulate == 0:\n",
        "                    self.optimizer.step()\n",
        "                    self.optimizer.zero_grad()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            pbar.set_postfix(loss=running_loss / (i + 1))\n",
        "\n",
        "            # Update EMA\n",
        "            self.ema.update(self.model)\n",
        "\n",
        "        return running_loss / len(loader)\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Main training loop\"\"\"\n",
        "        train_loader = self._load_dataset(train=True)\n",
        "        val_loader = self._load_dataset(train=False)\n",
        "\n",
        "        num_batches = len(train_loader)\n",
        "        warmup_iters = max(round(self.params.get('warmup_epochs', 3) * num_batches), 500)\n",
        "\n",
        "        # Create log file\n",
        "        csv_file = open('weights/training_log.csv', 'w', newline='')\n",
        "        writer = csv.DictWriter(csv_file, fieldnames=['epoch', 'train_loss', 'val_loss', 'mAP50', 'mAP'])\n",
        "        writer.writeheader()\n",
        "\n",
        "        print(f\"Starting training for {self.args.epochs} epochs...\")\n",
        "        print(f\"Training samples: {len(train_loader.dataset)}\")\n",
        "        print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
        "        print(f\"Batch size: {self.args.batch_size}, Accumulate: {self.accumulate}\")\n",
        "\n",
        "        for epoch in range(self.args.epochs):\n",
        "            # Train one epoch\n",
        "            train_loss = self.train_epoch(train_loader, epoch, warmup_iters, num_batches)\n",
        "\n",
        "            # Update learning rate\n",
        "            self.scheduler.step()\n",
        "\n",
        "            # Validate\n",
        "            val_loss, map50, map_val = self.validate(val_loader)\n",
        "\n",
        "            # Log results\n",
        "            writer.writerow({\n",
        "                'epoch': epoch + 1,\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_loss,\n",
        "                'mAP50': map50,\n",
        "                'mAP': map_val\n",
        "            })\n",
        "            csv_file.flush()\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{self.args.epochs} - \"\n",
        "                  f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
        "                  f\"mAP50: {map50:.4f}, mAP: {map_val:.4f}\")\n",
        "\n",
        "            # Save best model\n",
        "            if map_val > self.best_map:\n",
        "                self.best_map = map_val\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': self.ema.ema.state_dict(),\n",
        "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                    'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "                    'best_map': self.best_map,\n",
        "                    'params': self.params\n",
        "                }, \"weights/best_model.pth\")\n",
        "                print(f\"Saved best model with mAP: {map_val:.4f}\")\n",
        "\n",
        "            # Save latest model\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': self.ema.ema.state_dict(),\n",
        "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "                'best_map': self.best_map,\n",
        "                'params': self.params\n",
        "            }, \"weights/latest_model.pth\")\n",
        "\n",
        "            # Save checkpoint every 10 epochs\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': self.ema.ema.state_dict(),\n",
        "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                    'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "                    'best_map': self.best_map,\n",
        "                    'params': self.params\n",
        "                }, f\"weights/checkpoint_epoch_{epoch+1}.pth\")\n",
        "\n",
        "        csv_file.close()\n",
        "        print(f\"Training completed! Best mAP: {self.best_map:.4f}\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self, loader):\n",
        "        self.model.eval()  # Use regular model instead of EMA\n",
        "        val_loss = 0\n",
        "        preds = []\n",
        "        targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (images, labels, _) in enumerate(loader):\n",
        "                images = images.to(self.device, non_blocking=True).float() / 255.0\n",
        "                labels = labels.to(self.device)\n",
        "                targets.append(labels)\n",
        "\n",
        "                # Use regular model instead of EMA\n",
        "                outputs = self.model(images)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                preds.append(outputs)\n",
        "\n",
        "        map50, map_val = self.calculate_map(loader)\n",
        "        return val_loss / len(loader), map50, map_val\n",
        "\n",
        "    # @torch.no_grad()\n",
        "    # def validate(self, loader):\n",
        "    #   \"\"\"Validate the model\"\"\"\n",
        "    #   self.ema.ema.eval()\n",
        "    #   running_loss = 0.0\n",
        "\n",
        "    #   # Disable mixed precision for validation to avoid dtype issues\n",
        "    #   for images, targets, _ in tqdm.tqdm(loader, desc=\"Validating\"):\n",
        "    #       images = images.to(self.device).float() / 255.0  # Ensure float32\n",
        "    #       targets = targets.to(self.device)\n",
        "\n",
        "    #       # Use float32 for validation to avoid reparameterization issues\n",
        "    #       outputs = self.ema.ema(images.float())  # Force float32\n",
        "    #       loss = self.criterion(outputs, targets)\n",
        "\n",
        "    #       running_loss += loss.item()\n",
        "\n",
        "    #   # Calculate mAP (placeholder)\n",
        "    #   map50, map_val = self.calculate_map(loader)\n",
        "\n",
        "    #   return running_loss / len(loader), map50, map_val\n",
        "\n",
        "    def calculate_map(self, loader):\n",
        "        \"\"\"Calculate mAP - placeholder implementation\"\"\"\n",
        "        # This is a simplified version. You can integrate proper mAP calculation\n",
        "        # using your validation dataset and detection metrics\n",
        "        return 0.5, 0.3  # Placeholder values\n",
        "\n",
        "    def test(self):\n",
        "        \"\"\"Test the model\"\"\"\n",
        "        val_loader = self._load_dataset(train=False)\n",
        "        val_loss, map50, map_val = self.validate(val_loader)\n",
        "\n",
        "        print(f\"Test Results - Loss: {val_loss:.4f}, mAP50: {map50:.4f}, mAP: {map_val:.4f}\")\n",
        "        return map50, map_val\n",
        "\n",
        "# Simple configuration class for Colab\n",
        "class TrainingConfig:\n",
        "    def __init__(self):\n",
        "        self.input_size = 416\n",
        "        self.batch_size = 8\n",
        "        self.epochs = 50\n",
        "        self.train = True\n",
        "        self.test = False\n",
        "        self.resume = ''\n",
        "        self.data_path = '/kaggle/input/car-object-detection'\n",
        "\n",
        "# Default configuration\n",
        "default_config = {\n",
        "    'nc': 1,  # number of classes (car)\n",
        "    'multi_head': False,\n",
        "    'lr0': 0.01,\n",
        "    'lrf': 0.01,\n",
        "    'momentum': 0.937,\n",
        "    'weight_decay': 0.0005,\n",
        "    'warmup_epochs': 3,\n",
        "    'cls_loss': 0.5,\n",
        "    'box_loss': 7.5,\n",
        "    'dfl_loss': 1.5,\n",
        "    'data': {\n",
        "        'path': '/kaggle/input/car-object-detection'\n",
        "    }\n",
        "}\n",
        "\n",
        "def start_training(epochs=50, batch_size=16, resume_checkpoint=''):\n",
        "    \"\"\"\n",
        "    Start training in Google Colab\n",
        "\n",
        "    Args:\n",
        "        epochs (int): Number of training epochs\n",
        "        batch_size (int): Batch size for training\n",
        "        resume_checkpoint (str): Path to checkpoint to resume from\n",
        "    \"\"\"\n",
        "    # Create configuration\n",
        "    config = TrainingConfig()\n",
        "    config.epochs = epochs\n",
        "    config.batch_size = batch_size\n",
        "    config.resume = resume_checkpoint\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = YoloTrainer(config, default_config)\n",
        "\n",
        "    # Resume from checkpoint if specified\n",
        "    if resume_checkpoint and os.path.exists(resume_checkpoint):\n",
        "        print(f\"Resuming from checkpoint: {resume_checkpoint}\")\n",
        "        checkpoint = torch.load(resume_checkpoint, map_location=trainer.device)\n",
        "        trainer.ema.ema.load_state_dict(checkpoint['model_state_dict'])\n",
        "        trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        trainer.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        trainer.best_map = checkpoint.get('best_map', 0.0)\n",
        "        print(f\"Resumed from epoch {checkpoint['epoch']}, best mAP: {trainer.best_map:.4f}\")\n",
        "\n",
        "    # Start training\n",
        "    trainer.train()\n",
        "\n",
        "def start_testing(checkpoint_path='weights/best_model.pth'):\n",
        "    \"\"\"\n",
        "    Test the model in Google Colab\n",
        "\n",
        "    Args:\n",
        "        checkpoint_path (str): Path to model checkpoint to test\n",
        "    \"\"\"\n",
        "    config = TrainingConfig()\n",
        "    config.train = False\n",
        "    config.test = True\n",
        "\n",
        "    trainer = YoloTrainer(config, default_config)\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading model from: {checkpoint_path}\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=trainer.device)\n",
        "        trainer.ema.ema.load_state_dict(checkpoint['model_state_dict'])\n",
        "        trainer.best_map = checkpoint.get('best_map', 0.0)\n",
        "        print(f\"Loaded model from epoch {checkpoint['epoch']}, best mAP: {trainer.best_map:.4f}\")\n",
        "\n",
        "    trainer.test()\n",
        "\n",
        "# Simple function to show training progress\n",
        "def show_training_progress():\n",
        "    \"\"\"Show training progress from log file\"\"\"\n",
        "    if os.path.exists('weights/training_log.csv'):\n",
        "        import pandas as pd\n",
        "        df = pd.read_csv('weights/training_log.csv')\n",
        "        print(\"Training Progress:\")\n",
        "        print(df.tail(10))\n",
        "    else:\n",
        "        print(\"No training log found.\")\n",
        "\n",
        "# Usage examples for Colab:\n",
        "def example_usage():\n",
        "    \"\"\"\n",
        "    Example usage in Google Colab:\n",
        "\n",
        "    # Start training with default parameters\n",
        "    start_training()\n",
        "\n",
        "    # Start training with custom parameters\n",
        "    start_training(epochs=100, batch_size=8)\n",
        "\n",
        "    # Resume training from checkpoint\n",
        "    start_training(epochs=50, batch_size=16, resume_checkpoint='weights/checkpoint_epoch_10.pth')\n",
        "\n",
        "    # Test the model\n",
        "    start_testing('weights/best_model.pth')\n",
        "\n",
        "    # Show training progress\n",
        "    show_training_progress()\n",
        "    \"\"\"\n",
        "    print(example_usage.__doc__)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # This will run when executed directly in Colab\n",
        "    print(\"YOLOv8 Hybrid Trainer for Google Colab\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Show example usage\n",
        "    example_usage()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POh0Yx6elnF8",
        "outputId": "3d982dea-c4d7-402d-b8ef-2f88d8201fb0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLOv8 Hybrid Trainer for Google Colab\n",
            "==================================================\n",
            "\n",
            "    Example usage in Google Colab:\n",
            "    \n",
            "    # Start training with default parameters\n",
            "    start_training()\n",
            "    \n",
            "    # Start training with custom parameters\n",
            "    start_training(epochs=100, batch_size=8)\n",
            "    \n",
            "    # Resume training from checkpoint\n",
            "    start_training(epochs=50, batch_size=16, resume_checkpoint='weights/checkpoint_epoch_10.pth')\n",
            "    \n",
            "    # Test the model\n",
            "    start_testing('weights/best_model.pth')\n",
            "    \n",
            "    # Show training progress\n",
            "    show_training_progress()\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_training(epochs=50, batch_size=16)"
      ],
      "metadata": {
        "id": "lGiG5ZpGl3gJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77c7a5c1-e6b6-450e-e8fe-a7202bfd2632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Loading training dataset from: /kaggle/input/car-object-detection\n",
            "Found 1176 images\n",
            "Creating training dataset with 940 images...\n",
            "Loading cache from: ./dataset_cache/testing_images.cache\n",
            "Creating validation dataset with 236 images...\n",
            "Loading cache from: ./dataset_cache/testing_images.cache\n",
            "Training samples: 940\n",
            "Validation samples: 236\n",
            "Train batches: 59\n",
            "Val batches: 59\n",
            "Found 1176 images\n",
            "Creating training dataset with 940 images...\n",
            "Loading cache from: ./dataset_cache/testing_images.cache\n",
            "Creating validation dataset with 236 images...\n",
            "Loading cache from: ./dataset_cache/testing_images.cache\n",
            "Training samples: 940\n",
            "Validation samples: 236\n",
            "Train batches: 59\n",
            "Val batches: 59\n",
            "Starting training for 50 epochs...\n",
            "Training samples: 940\n",
            "Validation samples: 940\n",
            "Batch size: 16, Accumulate: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/50:  75%|███████▍  | 44/59 [00:53<00:14,  1.03it/s, loss=7.49e+5]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kY3BVIOU1f4z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}