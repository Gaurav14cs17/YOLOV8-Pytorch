# YOLOv8-RepNeXt

Hey there! ðŸ‘‹ Welcome to this PyTorch implementation of **YOLOv8** powered by the **RepNeXt architecture**. If you're looking for a fast, accurate object detection model that's easy to train and deploy, you're in the right place.

This project is inspired by the [RepNeXt paper](https://arxiv.org/abs/2406.16004) â€” a clever approach that uses multiple convolutional branches during training but fuses them into a single efficient layer at inference time. The result? You get the best of both worlds: rich feature learning during training and blazing-fast inference.

---

## What Makes This Special?

The magic lies in **structural reparameterization**. Here's the idea in plain terms:

- **During training**, the model uses 5 parallel branches (7Ã—7, 5Ã—5, 3Ã—3, 1Ã—1, and identity) to capture features at different scales
- **During inference**, all those branches get mathematically fused into a single 7Ã—7 convolution
- **The result**: Same accuracy, way faster inference â€” no tricks, just math!

![YOLOv8-RepNeXt Architecture](images/yolo_architecture.svg)

---

## RepNeXt Block â€” The Core Innovation

The RepNeXt block is where the magic happens. During training, it uses multiple parallel branches to learn rich features:

![RepNeXt Block](images/repnext_block.svg)

### How Reparameterization Works

At inference time, all those branches are mathematically fused into a single convolution. No accuracy loss, just pure speed:

![Reparameterization Process](images/reparameterization.svg)

---

## CSP Block Design

We use Cross Stage Partial (CSP) blocks with RepNeXt for better gradient flow and reduced computation:

![CSP Block](images/csp_block.svg)

---

## Getting Started

### Installation

Clone the repo and install dependencies:

```bash
git clone https://github.com/Gaurav14cs17/YOLOV8-Pytorch.git
cd YOLOV8-Pytorch
pip install -r requirements.txt
```

### Your First Model

Creating a model is straightforward:

```python
from model import yolo_v8_n, yolo_v8_s, yolo_v8_m

# Pick the size that fits your needs
model = yolo_v8_n(num_classes=80)   # Nano - fastest, good for edge devices
model = yolo_v8_s(num_classes=80)   # Small - nice balance
model = yolo_v8_m(num_classes=80)   # Medium - when you need more accuracy

# Test it out
import torch
x = torch.randn(1, 3, 640, 640)
outputs = model(x)
```

### Speed Up Inference

Before deploying, reparameterize the model to fuse all branches:

```python
model.eval()
model.reparameterize()

# Now it's faster!
with torch.no_grad():
    outputs = model(x)
```

---

## Training Your Own Model

### Standard Training

```bash
python train.py --train --epochs 100 --batch-size 16 --input-size 640
```

### Quantization-Aware Training

Want to deploy on edge devices? Use QAT for better performance on limited hardware:

```bash
python qtrain.py --train --epochs 20 --batch-size 32
```

Training outputs are saved to different folders:
- Standard training â†’ `weights/`
- Quantized training â†’ `weights_quant/`

### Dataset Setup

Organize your data in YOLO format:

```
dataset/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ train/
â”‚   â””â”€â”€ val/
â””â”€â”€ labels/
    â”œâ”€â”€ train/
    â””â”€â”€ val/
```

Each label file should have one line per object:
```
class_id x_center y_center width height
```
All coordinates are normalized to 0-1.

---

## Running Inference

### Standard Model

```bash
python inference.py \
    --weights weights/best.pt \
    --source path/to/image.jpg \
    --conf-thres 0.25 \
    --iou-thres 0.45 \
    --save
```

### Quantized Model

```bash
python qinference.py \
    --weights weights_quant/best.pt \
    --source path/to/image.jpg \
    --conf-thres 0.25 \
    --save
```

---

## Model Variants

Choose based on your speed/accuracy tradeoff:

| Variant | Parameters | Use Case |
|---------|------------|----------|
| nano | ~3M | Edge devices, real-time on CPU |
| tiny | ~5M | Mobile devices |
| small | ~11M | Good balance for most cases |
| medium | ~26M | When accuracy matters more |
| large | ~44M | High accuracy applications |
| xlarge | ~68M | Maximum accuracy |

---

## Configuration

All settings live in `config/config.yml`. Here are the key ones:

```yaml
# Model architecture
variant: 'small'        # nano, tiny, small, medium, large, xlarge
neck_type: 'fpn'        # fpn, lite, bifpn

# Training hyperparameters
lr0: 0.01
momentum: 0.937
weight_decay: 0.0005
epochs: 300

# Data augmentation
mosaic: 0.5            # Mosaic augmentation probability
mixup: 0.1             # MixUp augmentation probability
hsv_h: 0.015           # Hue shift
hsv_s: 0.7             # Saturation shift
hsv_v: 0.4             # Value shift

# Your classes
names:
  0: person
  1: bicycle
  # ... add your classes
```

---

## Project Structure

Here's how the code is organized:

```
YOLOV8-Pytorch/
â”œâ”€â”€ model/              # Core model architecture
â”‚   â”œâ”€â”€ repnext.py      # RepNeXt blocks & reparameterization
â”‚   â”œâ”€â”€ backbone.py     # Feature extractor
â”‚   â”œâ”€â”€ neck.py         # FPN/PANet for multi-scale fusion
â”‚   â”œâ”€â”€ head.py         # Detection head with DFL
â”‚   â””â”€â”€ yolo.py         # Complete YOLO model
â”‚
â”œâ”€â”€ qmodel/             # Quantized version for edge deployment
â”‚   â”œâ”€â”€ repnext.py      # Quantization-friendly RepNeXt
â”‚   â”œâ”€â”€ yolo.py         # Quantized YOLO model
â”‚   â””â”€â”€ pruning.py      # Model pruning utilities
â”‚
â”œâ”€â”€ dataloader/         # Data loading & augmentation
â”‚   â”œâ”€â”€ dataset.py      # Dataset class
â”‚   â”œâ”€â”€ augmentations.py# HSV, Flip, MixUp, etc.
â”‚   â””â”€â”€ mosaic.py       # Mosaic augmentation
â”‚
â”œâ”€â”€ utils/              # Helper functions
â”‚   â”œâ”€â”€ boxes.py        # Box operations, NMS
â”‚   â”œâ”€â”€ metrics.py      # mAP calculation
â”‚   â”œâ”€â”€ losses.py       # Loss functions
â”‚   â””â”€â”€ assigner.py     # Label assignment
â”‚
â”œâ”€â”€ train.py            # Standard training script
â”œâ”€â”€ qtrain.py           # Quantization-aware training
â”œâ”€â”€ inference.py        # Run inference with standard model
â”œâ”€â”€ qinference.py       # Run inference with quantized model
â””â”€â”€ config/config.yml   # All your settings
```

---

## Features at a Glance

**Architecture**
- Multi-branch RepNeXt blocks for rich feature learning
- Structural reparameterization for fast inference
- Decoupled detection head with DFL
- Task-aligned label assignment

**Training**
- Mosaic and MixUp augmentation
- EMA (Exponential Moving Average) for stable training
- Mixed precision (FP16) support
- Multi-GPU training with DDP

**Optimization**
- Quantization-aware training for edge deployment
- Model pruning support
- Conv-BN fusion for faster inference

---

## Acknowledgments

This project builds on great work from:
- [YOLO Series Paper](https://arxiv.org/abs/2304.00501) â€” *"YOLOv1 to YOLOv8, The Rise of Real-Time Object Detection"*
- [RepNeXt Paper](https://arxiv.org/abs/2406.16004) â€” *"RepNeXt: A Fast Multi-Scale CNN using Structural Reparameterization"*
- [RepNeXt GitHub](https://github.com/suous/RepNeXt) â€” reference implementation

---

## License

MIT License â€” use it however you'd like!

---

## Sample Results

![Sample Detection](images/image_2.jpg)


